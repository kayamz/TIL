# TIL (2024-09-03)

## Day 2: Understanding Transformer Models

### What is a Transformer Model?
- **Transformer 모델**: 트랜스포머는 자연어 처리(NLP) 작업에서 혁신을 일으킨 딥러닝 모델이다. 주로 시퀀스-투-시퀀스(Sequence-to-Sequence) 문제를 해결하기 위해 개발되었으며, 기존의 RNN(Recurrent Neural Network)과 달리 병렬 처리가 가능하다. 트랜스포머는 인코더(Encoder)와 디코더(Decoder)로 구성되며, 셀프 어텐션(self-attention) 메커니즘을 활용하여 입력 데이터 내의 연관성을 학습한다.

### Key Components of Transformer
- **Encoder**: 인코더는 입력 시퀀스를 처리하여 고차원 벡터 표현으로 변환한다. 여러 개의 인코더 레이어가 쌓여 있으며, 각 레이어는 셀프 어텐션과 피드포워드 뉴럴 네트워크로 구성된다.
  
- **Decoder**: 디코더는 인코더의 출력을 받아 최종 출력 시퀀스를 생성한다. 디코더 역시 여러 개의 레이어로 구성되며, 셀프 어텐션 외에 인코더의 출력을 참고하는 인코더-디코더 어텐션 메커니즘을 포함한다.

- **Self-Attention**: 셀프 어텐션은 입력 시퀀스의 각 요소가 다른 모든 요소와 어떻게 연관되는지를 계산한다. 이를 통해 모델이 문맥 정보를 잘 이해하게 한다. 셀프 어텐션은 입력의 각 단어에 대해, 다른 모든 단어들과의 관계를 가중합하여 표현을 업데이트한다.

- **Positional Encoding**: 트랜스포머는 시퀀스의 순서 정보를 유지하기 위해 위치 인코딩(Positional Encoding)을 추가한다. 이는 입력에 대한 위치 정보를 벡터로 표현한 후, 해당 벡터를 입력 임베딩에 더하는 방식으로 구현된다.

### How Does Transformer Work?
- **Step 1**: 입력 시퀀스가 인코더에 들어가고, 인코더는 각 단어의 셀프 어텐션을 계산하여 고차원 표현을 생성한다.
- **Step 2**: 디코더는 이 인코더의 출력을 사용하여 출력 시퀀스를 예측한다. 디코더는 또한 이전에 생성된 출력 토큰을 입력으로 받아 셀프 어텐션을 적용한다.
- **Step 3**: 최종적으로 디코더는 완성된 출력 시퀀스를 생성하고, 이는 번역, 텍스트 생성 등의 작업에 사용된다.

### Real-World Applications
- **Machine Translation**: 트랜스포머는 번역 시스템에서 매우 중요한 역할을 한다. Google 번역과 같은 시스템이 이 모델을 기반으로 동작한다.
  
- **Text Summarization**: 긴 문서나 기사의 요약본을 자동으로 생성하는 데 사용된다. 트랜스포머는 문맥을 잘 이해하기 때문에 자연스러운 요약을 제공한다.

- **Text Generation**: GPT 시리즈와 같은 모델은 트랜스포머 구조를 기반으로 하며, 매우 현실적인 텍스트를 생성하는 데 탁월하다.

### Learning Resources
- [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)

### Summary
오늘은 트랜스포머 모델의 기본 개념과 구조에 대해 배웠다. 이 모델은 LLM의 핵심 기반이 되는 기술로, 셀프 어텐션 메커니즘과 인코더-디코더 구조를 이해하는 것이 중요하다. 앞으로는 트랜스포머를 더욱 깊이 탐구하고, 이를 활용한 모델을 직접 구현해보는 것을 목표로 한다.

---

**Commit message**: `Add Day 2: Understanding Transformer Models`
